{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583d85db",
   "metadata": {},
   "source": [
    "Homework 1, Machine Learning, Fall 2021\n",
    "==============================\n",
    "\n",
    "Concepts of learning\n",
    "-----------------------------\n",
    "\n",
    "1. regression\n",
    "2. regression\n",
    "3. classification, regression\n",
    "4. regression\n",
    "5. classification\n",
    "6. pattern mining\n",
    "7. clustering\n",
    "8. pattern mining\n",
    "9. density estimation\n",
    "10. probability estimation\n",
    "11. conditional probability estimation, regression\n",
    "12. ranking\n",
    "13. clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80efe803",
   "metadata": {},
   "source": [
    "ROC and AUC\n",
    "--------------------\n",
    "\n",
    "age |course |likeStats ||Y\n",
    "----|-------|----------||--\n",
    "20  |1      |0         ||0\n",
    "18  |1      |1         ||0\n",
    "15  |0      |1         ||1\n",
    "16  |0      |0         ||1\n",
    "17  |1      |1         ||0\n",
    "21  |1      |0         ||0\n",
    "23  |1      |0         ||1\n",
    "15  |1      |1         ||0\n",
    "17  |0      |1         ||1\n",
    "17  |1      |0         ||0\n",
    "\n",
    "(a) $$g(x) = \\theta^Tx + \\theta_0$$\n",
    "    1) $$x = [20, 1, 0]$$\n",
    "    $$g([20, 1, 0]) = (0.05)*(20) -3 + 0.3 = -1.7$$\n",
    "    2) $$x = [18, 1, 1]$$\n",
    "    $$g([18, 1, 1]) = (0.05)*(18) -3 + 2.5 + 0.3 = 0.7$$\n",
    "    3) $$x = [15, 0, 1]$$\n",
    "    $$g([15, 0, 1]) = (0.05)*(15) -3 * 0 + 2.5 + 0.3 = 3.55$$\n",
    "    4)$$x = [16, 0, 0]$$\n",
    "    $$g([16, 0, 0]) = (0.05)*(16) + 0.3 = 1.1$$\n",
    "    5)$$x = [17, 1, 1]$$\n",
    "    $$g([17, 1, 1]) = (0.05)*(17) -3 + 2.5 + 0.3 = 0.65$$\n",
    "    6)$$x = [21, 1, 0]$$\n",
    "    $$g([21, 1, 0]) = (0.05)*(21) -3 + 0.3 = -1.65$$\n",
    "    7)$$x = [23, 1, 0]$$\n",
    "    $$g([23, 1, 0]) = (0.05)*(23) -3 + 0.3 = -1.55$$\n",
    "    8)$$x = [15, 1, 1]$$\n",
    "    $$g([15, 1, 1]) = (0.05)*(15) -3 + 2.5 + 0.3 = 0.55$$\n",
    "    9)$$x = [17, 0, 1]$$\n",
    "    $$g([17, 0, 1]) = (0.05)*(17) + 2.5 + 0.3 = 3.65$$\n",
    "    10)$$x = [17, 1, 0]$$\n",
    "    $$g([17, 1, 0]) = (0.05)*(17) -3 + 0.3 = -1.85$$\n",
    "    \n",
    "    When counting misclassification data by setting the threshold as one of the g(x) values one by one, it turned out that there is only one misclassification data with the threshold 1.1 like the table below.\n",
    "Y |$\\hat{Y}$\n",
    "--|---------\n",
    "0 | 0\n",
    "0 | 0\n",
    "1 | 1\n",
    "1 | 1\n",
    "0 | 0\n",
    "0 | 0\n",
    "1 | 0\n",
    "0 | 0\n",
    "1 | 1\n",
    "0 | 0\n",
    "\n",
    "Misclassification error: \n",
    "${1 \\over 10} = 0.1 = 10%$\n",
    "\n",
    "Also, when checking misclassification one by one, it can be noticed that there is just 1 misclassiciation data for the threshold from 0.7 to 1.1. Therefore, choices of threshold to minimize misclassification error is $(0.7,1.1]$.\n",
    "\n",
    "(b) $$f(x) = \\sigma(\\theta^Tx + \\theta_0)$$ where $$\\sigma = (1 + e^{-z})^-1$$\n",
    "    1) $$x = [20, 1, 0]$$\n",
    "    $$g([20, 1, 0]) = 0.1545$$\n",
    "    2) $$x = [18, 1, 1]$$\n",
    "    $$g([18, 1, 1]) = 0.6682$$\n",
    "    3) $$x = [15, 0, 1]$$\n",
    "    $$g([15, 0, 1]) = 0.9721$$\n",
    "    4) $$x = [16, 0, 0]$$\n",
    "    $$g([16, 0, 0]) = 0.7503$$\n",
    "    5) $$x = [17, 1, 1]$$\n",
    "    $$g([17, 1, 1]) = 0.6570$$\n",
    "    6) $$x = [21, 1, 0]$$\n",
    "    $$g([21, 1, 0]) = 0.1611$$\n",
    "    7) $$x = [23, 1, 0]$$\n",
    "    $$g([23, 1, 0]) = 0.1751$$\n",
    "    8) $$x = [15, 1, 1]$$\n",
    "    $$g([15, 1, 1]) = 0.6341$$\n",
    "    9) $$x = [17, 0, 1]$$\n",
    "    $$g([17, 0, 1]) = 0.9747$$\n",
    "    10) $$x = [17, 0, 1]$$\n",
    "    $$g([17, 0, 1]) = 0.1359$$\n",
    "    \n",
    "    If misclassification data is counted by setting the threshold with f(x) values, it turned out that 0.7503 is the threshold to minimize misclassification error. When 0.7503 is the threshold, there is 9 right classification out of 10, which has 0.1 = 10% misclassification error. Therefore, 0.7503 is the appropriate threshold to minimize misclassification error because it has the lowest misclassification error. The table for classification is below.\n",
    "    \n",
    "Y |$\\hat{Y}$\n",
    "--|---------\n",
    "0 | 0\n",
    "0 | 0\n",
    "1 | 1\n",
    "1 | 1\n",
    "0 | 0\n",
    "0 | 0\n",
    "1 | 0\n",
    "0 | 0\n",
    "1 | 1\n",
    "0 | 0\n",
    "\n",
    "\n",
    "**Confusion Matrix**\n",
    "|           |Y=1 |Y=0|\n",
    "|-----------|----|---|\n",
    "|$\\hat{Y}$=1|3   |0  |\n",
    "|$\\hat{Y}$=0|1   |6  |\n",
    "\n",
    "**Percision**\n",
    "TP/#predicted postivive = TP/(TP+FP) = ${3 \\over 3+0}$\n",
    "\n",
    "**Recall (=sensitivity, TPR)**\n",
    "TP/#Pos = TP/(TP+FN) = ${3 \\over 3+1}$\n",
    "\n",
    "**F1 Score**\n",
    "F1 = 2 * Precision * Recall/(Precision + Recall) = ${2 * (1 * 0.75) \\over 1+0.75}$ = 0.857\n",
    "\n",
    "(c) Matlab code for 2.2 (c) is attached at the end of the document\n",
    "\n",
    "(d) There is no difference between the ROC curve of any function f(x) and the ROC curve of h(f(x)) because ROC curve is originally always monotonic. It does not decrease in y direction along increasing x. Therefore, these two options will show the same ROC curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5121be",
   "metadata": {},
   "source": [
    "Information Theroy\n",
    "---------------------------\n",
    "\n",
    "3.1) $$Gain(X,X) := I(X,Y) := H(X) - H(X|X) = H(X)$$\n",
    "\n",
    "In information gain, subtraction term is for the reduction of uncertainty about X. But, X given X is not giving any conditional information, which means $H(X|X) = 0$.\n",
    "Therefore, the expected information gain between X and itself is just $H(X)$.\n",
    "\n",
    "3.2) \n",
    "\n",
    "1. Each events happening separely\n",
    "\n",
    "$$P((X|E_2)|(E_1|E_2)) = \\frac{P((X|E_2), (E_1|E_2))}{P(E_1|E_2)}=\\frac{P(X,E_1|E_2)}{P(E_1|E_2)} = \\frac{P(X,E_1,E_2)}{P(E_1|E_2)P(E_2)} = \\frac{P(X,E_1,E_2)}{P(E_1)P(E_2)}$$\n",
    "\n",
    "\n",
    "\n",
    "By Bayes' theorem which is $$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$, the equation, $$\\frac{P(X,E_1|E_2)}{P(E_1|E_2)} = \\frac{P(X,E_1,E_2)}{P(E_1|E_2)P(E_2)}$$ can be proved. The reason why we got final term from third term is because $E_1$ and $E_2$ are independent event. Therefore, $P(E_1|E_2) = P(E_1)$.\n",
    "\n",
    "2. events happening altogether\n",
    "\n",
    "$$P(X|E_1,E_2) = \\frac{P(X,E_1,E_2)}{P(E_1,E_2)} = \\frac{P(X,E_1,E_2)}{P(E_1)P(E_2)}$$\n",
    "\n",
    "Again, event $E_1$ and $E_2$ are independent events, so from this we can get $$P(E_1,E_2) = P(E_1)P(E_2)$$.\n",
    "\n",
    "Because the information gain for first two events ($E_1, E_2$) is same for each case, for the rest of all events it's proved that the total information gain of each of these events happending separately is the same as the information gain of all these events happening together. \n",
    "\n",
    "3.3) $$I(X,Y) = H(Y) - H(Y|X)$$\n",
    "$$I(X,Y) = H(Y) - H(Y|X) \\geq 0$$\n",
    "$$H(Y) \\geq H(Y|X)$$\n",
    "\n",
    "For the right hand side $H(Y|X)$, in the maximum case Y and X are independent. \n",
    "If these two are independent, $H(Y|X) = H(Y)$.It will still meet the equation with $H(Y) \\geq H(Y)$. Therefore, $I(X,Y) \\geq 0$ is true.\n",
    "\n",
    "3.4) $$Z = X + Y$$\n",
    "$$P(Z|X) = P(Z-X|X) = P(Y|X)$$\n",
    "$$P(Z|X) = -\\sum\\limits_{x}\\sum\\limits_{z}P(z|x)\\log_{10} P(z|x)$$\n",
    "Because P(z|x) = P(y|x),\n",
    "\n",
    "$$ = -\\sum\\limits_{x}P(x){\\sum\\limits_{y}P(y|x)\\log_{10}P(y|x)} = H(Y|X)$$\n",
    "\n",
    "(b)\n",
    "\n",
    "1. $H(Y) \\leq H(Z)$\n",
    "\n",
    "    From 3.3 we can get $I(X,Z)\\geq0$.\n",
    "    $$H(Z)-H(Z|X) \\geq 0$$\n",
    "    $$H(Z) \\geq H(Z|X)$$\n",
    "\n",
    "    According to 3.4(a), $H(Z|X) = H(Y|X)$.\n",
    "    $$ \\therefore H(Z) \\geq H(Y|X)$$\n",
    "    Because X and Y are independent, $H(Y|X) = H(Y)$.\n",
    "    $$ \\therefore H(Z) \\geq H(Y)$$\n",
    "    \n",
    "    \n",
    "2. $H(X) \\leq H(Z)$\n",
    "\n",
    "    From 3.3 again, $I(Y,Z) \\geq 0$.\n",
    "    $$H(Z) - H(Z|Y) \\geq 0$$\n",
    "    $$H(Z) \\geq H(Z|Y)$$\n",
    "    \n",
    "    According to 3.4(a), $H(Z|Y) = H(X|Y)$.\n",
    "    $$ \\therefore H(Z) \\geq H(X|Y)$$\n",
    "    Because X and Y are independent, $H(X|Y) = H(X)$.\n",
    "    $$ \\therefore H(Z) \\geq H(X) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cf8da",
   "metadata": {},
   "source": [
    "Decision Tree\n",
    "-------------------\n",
    "\n",
    "4.1)\n",
    "\n",
    "(a) To see which scenario is the best feature to split on first, compare Gini reduction for all three scenarios.\n",
    "Before getting Gini reduction, get the common Gini index for the entire cases. \n",
    "\n",
    "$$G.I = 2p(1-p) = 2({1 \\over 2})({1 \\over 2})$$\n",
    "$$N=10$$\n",
    "\n",
    "1. splitting $X_1$ first\n",
    "    $$G.I_1=2({0 \\over 4})(1-{4 \\over 4})=0$$\n",
    "    $$G.I_2=2({5 \\over 6})(1-{5 \\over 6})={5 \\over 18}$$\n",
    "    $$G.R={1 \\over 2}-({4 \\over 10}*0+{6 \\over 10}*{5 \\over 18})=0.3333$$\n",
    "    \n",
    "2. splitting $X_2$ first\n",
    "    $$G.I_1=2({3 \\over 5})(1-{3 \\over 5})={12 \\over 25}$$\n",
    "    $$G.I_2=2({2 \\over 5})(1-{2 \\over 5})={12 \\over 25}$$\n",
    "    $$G.R={1 \\over 2}-({5 \\over 10}*{12 \\over 25}+{5 \\over 10}*{12 \\over 25})=0.02$$\n",
    "    \n",
    "3. splitting $X_3$ first\n",
    "    $$G.I_1=2({4 \\over 6})(1-{4 \\over 6})={4 \\over 9}$$\n",
    "    $$G.I_2=2({1 \\over 4})(1-{1 \\over 4})={3 \\over 8}$$\n",
    "    $$G.R={1 \\over 2}-({6 \\over 10}*{4 \\over 9}+{4 \\over 10}*{3 \\over 8})=0.0833$$\n",
    "    \n",
    "With the same Gini index, the gini reduction is going to be increased as small as the sum of each gini index of nodes. Therefore, the biggest Gini reduction is the first scenario which is splitting $X_1$ first.\n",
    "\n",
    "(b) Compare information gain for each scenarios with information gain formula\n",
    "\n",
    "1. splitting $X_1$ first\n",
    "    $$Gain(S,A) = H([{1 \\over 2},{1 \\over 2}])-[{4 \\over 10}H([{0 \\over 4},{4 \\over 4}])+{6 \\over 10}H([{5 \\over 6},{1 \\over 6}])]$$\n",
    "    $$ = 1-[0+{3 \\over 5}(-{5 \\over 6}\\log_{2}{5 \\over 6}-{1 \\over 6}\\log_{2}{1 \\over 6})]$$\n",
    "    $$=0.60998 \\approx 0.61$$\n",
    "    \n",
    "2. splitting $X_2$ first\n",
    "    $$Gain(S,A) = H([{1 \\over 2},{1 \\over 2}])-[{5 \\over 10}H([{3 \\over 5},{2 \\over 5}])+{5 \\over 10}H([{2 \\over 5},{3 \\over 5}])]$$\n",
    "    $$ = 1-{1 \\over 2}[-{3 \\over 5}\\log_{2}{3 \\over 5}-{2 \\over 5}\\log_{2}{2 \\over 5}-{3 \\over 5}\\log_{2}{3 \\over 5}-{2 \\over 5}\\log_{2}{2 \\over 5}]$$\n",
    "    $$=0.029$$\n",
    "    \n",
    "3. splitting $X_3$ first\n",
    "    $$Gain(S,A) = H([{1 \\over 2},{1 \\over 2}])-[{6 \\over 10}H([{4 \\over 6},{2 \\over 6}])+{4 \\over 10}H([{1 \\over 4},{3 \\over 4}])]$$\n",
    "    $$ = 1-[{6 \\over 10}(-{4 \\over 6}\\log_{2}{4 \\over 6}-{2 \\over 6}\\log_{2}{2 \\over 6})+{4 \\over 10}(-{1 \\over 4}\\log_{2}{1 \\over 4}-{3 \\over 4}\\log_{2}{3 \\over 4})]$$\n",
    "    $$=0.1245$$\n",
    "    \n",
    "Because information is better as its value is bigger, splitting according to information gain also goes with first scenario, splitting $X_1$ first.\n",
    "\n",
    "4.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e7f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
